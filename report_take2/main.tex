\documentclass[11pt]{article}

\usepackage{amsmath, amsthm, amssymb, amsfonts}
\usepackage{thmtools}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{geometry}
\usepackage{float}
\usepackage{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{framed}
\usepackage[dvipsnames]{xcolor}
\usepackage[most]{tcolorbox}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{indentfirst}
\usepackage[export]{adjustbox}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{fancyhdr}
\usepackage{cleveref}

\definecolor{LightGray}{RGB}{230, 230, 242}
\definecolor{LightOrange}{RGB}{255, 239, 213}
\definecolor{LightGreen}{RGB}{240, 255, 240}
\definecolor{MainBlue}{RGB}{0, 60, 113}

\newcommand{\HRule}[1]{\rule{\linewidth}{#1}}
\newcommand{\code}[1]{\texttt{#1}}

\geometry{
  textheight=24.5cm,
  textwidth=15.75cm,
  top=2cm,
  headheight=12pt,
  headsep=20pt,
  footskip=20pt,
  marginparwidth=1.5cm
}

\setstretch{1.15}

\pagestyle{fancy}
\fancyhf{}
\fancyfoot[C]{\thepage}

\begin{document}

\begin{center}
  \begin{figure}
    \includegraphics[scale = 0.3, left]{img/IST_A.eps} % IST logo
    \end{figure}
  \LARGE{ \normalsize \textsc{} \\
  [2.0cm] 
  \LARGE{ \LARGE \textsc{Aprendizagem}} \\
  [1cm]
  \LARGE{ \LARGE \textsc{LEIC IST-UL}} \\
  [1cm]
  \HRule{1.5pt} \\
  [0.4cm]
  \LARGE \textbf{\uppercase{Relat√≥rio - Merit Prize Challenge}}
  \HRule{1.5pt}
  \\ [2.5cm]
  }
\end{center}

\begin{flushleft}
  \textbf{\LARGE Grupo 10:}
\end{flushleft}

\begin{center}
  \begin{minipage}{0.7\textwidth}
      \begin{flushleft}
        \large Gabriel Ferreira \\
        \large  Irell Zane
      \end{flushleft}
  \end{minipage}%
  \begin{minipage}{0.3\textwidth}
      \begin{flushright}
        \large 107030\\
        \large 107161
      \end{flushright}
  \end{minipage}
\end{center}

\hfill

\begin{center}
  \vspace{4cm}
  \date \large \bf  2024/2025 -- 1st Semester, P1
\end{center}

\setcounter{page}{0}
\thispagestyle{empty}
\newpage

\section{Logistic Regression Analysis}

We conducted a comprehensive analysis of logistic regression performance using both raw and standardized data to evaluate the impact of preprocessing on model effectiveness. The standardization process involved scaling the features to have zero mean and unit variance, which helps prevent features with larger scales from dominating the model's learning process.

\begin{table}[H]
\centering
\caption{Logistic Regression Performance Comparison}
\label{tab:log-accuracy}
\begin{tabular}{@{}lcc@{}}
\toprule
Preprocessing Method & Testing Accuracy & Training Accuracy \\ \midrule
Without Scaling & 97.66\% & 95.98\% \\
Standardized Data & 98.25\% & 98.74\% \\
\bottomrule
\end{tabular}
\end{table}

The results demonstrate that standardization led to improved performance across both training and testing datasets. The standardized model achieved a testing accuracy of 98.25\%, representing a significant improvement over the non-standardized version. This improvement suggests that feature scaling is crucial for this dataset, likely due to the varying scales of the original features.

\section{EM Clustering Analysis}

Our implementation of Expectation-Maximization (EM) clustering utilized \code{scikit-learn}'s \code{GaussianMixture} model. We evaluated clustering quality using silhouette scores across different numbers of clusters ($k \in [2,10]$) to determine the optimal cluster count for our dataset.

\begin{figure}[H]
  \centering
  \begin{minipage}{0.48\textwidth}
    \includegraphics[width=\linewidth]{img/Silhouettes.png}
    \caption{Silhouette scores vs. cluster count}
    \label{fig:silhouette-score}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth}
    \includegraphics[width=\linewidth]{img/EMProbMapAccuracy.png}
    \caption{Logistic Reggression on the EM Probability Map}
    \label{fig:em-accuracy}
  \end{minipage}
\end{figure}

The silhouette analysis revealed several important trends, see Figure \ref{fig:silhouette-score}:
\begin{itemize}
  \setlength{\itemsep}{0pt}
    \item A clear declining trend in clustering quality as the number of clusters increases;
    \item Growing disparity between training and testing silhouette scores with
    higher cluster counts, suggesting a loss in the generalization capacity of
    the clustering solution;
    \item Optimal clustering achieved with k=2, yielding silhouette scores of 0.626 (testing) and 0.667 (training);
\end{itemize}

\setcounter{section}{3}

\section{Logistic Reggression on EM Probabilities Analysis}

Upon analyzing Figure \ref{fig:em-accuracy}, the relationship between the number of clusters 
($k$) and both performance metrics - the Silhouette scores and
Logistic Regression accuracy on the mapped dataset - we can see
that there is a notable relation: For low numbers of clusters,
such as 2 and 3, both metrics show high values, and as the $k$
increases, there is a sharp decline in both the silhouette score
and the classification accuracy of the logistic model, which
seem to follow the same trajectory, suggesting that a higher
quality of clustering, as measured by the Silhouette score, is
conducive to a better logistic regression model.

Using $k=2$ or $k=3$ the EM algorithm effectively serves as a
dimensionality reduction technique, transforming the original
high-dimensional feature space (30 features) into a lower
dimensional feature space, while maintaining a high accuracy
on the logistic regression model.

\section{RBF Network Implementation}

Our Radial Basis Function (RBF) Network implementation consisted of three key layers:

\begin{enumerate}
    \item Input layer matching the dimension of observations;
    \item Hidden layer corresponding to the number of clusters;
    \item Output layer with a single neuron for binary classification;
\end{enumerate}

Before training an RBF network we need to choose a Radial Basis Function. The 
most commonly used is the Gaussian and since we are working with EM Clustering
it would not make sense to measure just the distance between the observation and
the mean. We want the distance between the point of the observation and the
distribution of cluster 1 and 2, the distance between a point and a distribution 
is the Mahalanobis distance.

The Gaussian Radial Basis Function employed in our network is thus defined as:
\begin{equation}
  \phi_k(x)=exp(-\gamma \cdot (x-\mu)^T\Sigma_k^{-1}(x-\mu))
\end{equation}

Initial implementation with the standard\footnote{ 
  As suggested by Wichert and Sa-Couto (2021, Chapter 10)
} 
$\gamma=\frac{1}{2}$ yielded low accuracies:
\begin{itemize}
    \item Train Accuracy: 62.56\%;
    \item Test Accuracy: 63.15\%;
\end{itemize}

This low accuracy was due to RBF values being in wildly different scales, most of
them too small, see Figure \ref{fig:rbf-dist-big}.
With so many of them being near zero, and some so small that their double
precision floating point representation is zero,
the model is unable to learn properly, and because of this, the model
always made the same prediction regardless of input.

\begin{figure}[H]
  \centering
  \begin{minipage}{0.48\textwidth}
      \includegraphics[width=\linewidth]{img/RBF_Activation_500.png}
      \caption{Hidden layer activation distribution ($\gamma=0.5$)}
      \label{fig:rbf-dist-big}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth}
      \includegraphics[width=\linewidth]{img/RBF_Activation_15.png}
      \caption{Hidden layer activation distribution ($\gamma=0.015$)}
      \label{fig:rbf-dist-small}
  \end{minipage}
\end{figure}

To address this challenge we attempted to increase the scale by decreasing the
gamma value, trying many different values, see Figure \ref{fig:rbf-gamma-gaus}
and it seemed to be effective as a means to attenuate the scale
differences, see Figure \ref{fig:rbf-dist-small}. We also considered that
another good solution would be normalizing the results of the hidden layer,
since this would guarantee that at least one neuron had a non near-zero value,
meaning the regression model would be able to update its weights more
effectively.


\begin{figure}[H]
  \centering
  \begin{minipage}{0.48\textwidth}
    \includegraphics[width=\linewidth]{img/GammaHyperparameterTuning.png}
    \caption{RBF Network gamma parameter tuning results}
    \label{fig:rbf-gamma-gaus}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth}
    \includegraphics[width=\linewidth]{img/NormGammaHyperparameterTuning.png}
    \caption{Normalized RBF Network gamma tuning results}
  \end{minipage}
\end{figure}

The Normalized RBF (NRBF) implementation achieved several improvements:
\begin{itemize}
  \setlength{\itemsep}{0pt}
    \item Increased testing accuracy from 92.40\% ($\gamma=0.015$) to 92.96\% with $\gamma=0.5$;
    \item Reduced sensitivity to gamma parameter selection, meaning that the
    standard $\gamma$ of 0.5 can be chosen, making time costly hyperparameter tuning
    no longer a necessity;
    \item More stable and consistent performance across different configurations;
    \item Better distribution of activation values, leading to improved learning;
\end{itemize}

% The NRBF distributions are discriminative in that there is as much small
% activations as there are large, (Suggesting points that are part of a cluster
% and points that are not):
%
% \begin{figure}[H]
%   \centering
%   \begin{subfigure}{0.45\linewidth}
%       \centering
%       \includegraphics[width=\linewidth]{img/NRBF_Activation_500.png}
%       \caption{$\gamma=0.5$}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}{0.45\linewidth}
%       \centering
%       \includegraphics[width=\linewidth]{img/NRBF_Activation_16.png}
%       \caption{$\gamma=0.036$}
%       \label{fig:nrbf-dist-16}
%   \end{subfigure}
%   \caption{Hidden layer activation value distribution}
%   \label{fig:nrbf-dist}
% \end{figure}

We also explored alternative basis functions. Here's the Linear RBF:
\begin{equation}
  \phi_k(x)=\gamma \cdot (x-\mu)^T\Sigma_k^{-1}(x-\mu)
\end{equation}

\begin{figure}[H]
  \centering
  \begin{minipage}{0.48\textwidth}
    \includegraphics[width=\linewidth]{img/LinearRBF.png}
    \caption{Linear RBF performance analysis}
  \end{minipage}
  \hfill
  \begin{minipage}{0.48\textwidth}
    \includegraphics[width=\linewidth]{img/LinearNRBF.png}
    \caption{Normalized Linear RBF performance}
  \end{minipage}
\end{figure}

The Linear RBF implementation showed promising results, outperforming the Gaussian RBF in terms of both accuracy and stability.

\section{Comparative Analysis and Conclusions}

Our comprehensive evaluation of different approaches revealed several key insights about both the models and the underlying data structure:

\begin{table}[H]
\centering
\caption{Model Performance Summary}
\begin{tabular}{@{}lc@{}}
\toprule
Model                                           & Testing Accuracy \\ \midrule
Logistic Regression (Standard Scaling)          & 98.25\%          \\
EM Probability Mapping ($k=2$)                  & 92.40\%          \\
Gaussian RBF Network (best $\gamma$ parameter)  & 92.40\%          \\
Gaussian NRBF Network                           & 92.96\%          \\
Linear RBF Network                              & 92.40\%          \\
Linear NRBF Network                             & 93.57\%          \\ \bottomrule
\end{tabular}
\end{table}

Key findings from our analysis:
\begin{itemize}
  \setlength{\itemsep}{0pt}
    \item Logistic regression's high accuracy (98.25\%) suggests predominantly linear decision boundaries in the feature space
    \item EM clustering successfully reduced dimensionality while preserving most of the discriminative information
    \item The normalized RBF approach demonstrated robust performance across various parameter settings
    \item Preprocessing and normalization proved essential for optimal performance across all methods
    \item The dataset exhibits natural binary structure, aligning well with the classification objective
\end{itemize}

\newpage

The practical implications of our findings include:
\begin{enumerate}
  \setlength{\itemsep}{0pt}
    \item EM clustering with k=2 provides an effective approach for dimensionality reduction while maintaining good classification performance
    \item Normalized RBF networks offer stable performance without requiring extensive parameter tuning
    \item Feature standardization consistently improves model performance across different approaches
    \item The choice between methods involves a trade-off between accuracy and computational complexity
\end{enumerate}

\begin{thebibliography}{99}

\bibitem{journey} 
Luis Sa-Couto and Andreas Miroslaus Wichert, 
\textit{Machine Learning - A Journey To Deep Learning: With Exercises And Answers}, 
International series of monographs on physics, 
World Scientific Pub Co Inc, 2021, ISBN: 9789811234057.

\end{thebibliography}

\end{document}